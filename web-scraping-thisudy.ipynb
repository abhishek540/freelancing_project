{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Importing the Libraries\nimport pandas as pd\nimport numpy as np\n\nfrom bs4 import BeautifulSoup\nimport requests\n\nimport time\n\nfrom IPython.display import Audio\nsound_file = './beep.wav'\n\nimport sys\nfrom termcolor import colored, cprint","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Audio(sound_file, autoplay=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def page_soup(page_number):\n    url = \"https://thistudy.com/index.php?board=162.\"+str(page_number*20)\n    #Making a GET requests to fetch the raw HTML content\n    # headers = {\n    #     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n    # }\n    # headers={\n    #   'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n    #   'Accept-Language':'zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7',\n    #   'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'\n    # }\n\n    r = requests.get(url)\n    c = r.content\n    #Parsing the HTML content\n    soup = BeautifulSoup(c,\"html.parser\")\n    \n    while soup.find(\"title\").text =='Verify, Please!':\n        alert = \"!\"*20,\"page_number \",str(page_number),\"\\t Captcha Error\",\"!\"*20\n        alert = colored(alert,\"red\",attrs=[\"reverse\",\"blink\"])\n        print(alert)\n        Audio(sound_file, autoplay=True)\n        time.sleep(30)\n        #Again Request That Page\n        r = requests.get(url)\n        c = r.content\n        #Parsing the HTML content\n        soup = BeautifulSoup(c,\"html.parser\")\n    \n        \n    text = \"Page Number\\t\"+str(page_number)+\"...\"\n    \n    text_color = colored(text, 'green', attrs=['reverse', 'blink'])\n    print(text_color)\n    \n    \n    return soup\n    \ndef table_soup(soup):\n    table = soup.find(\"table\")\n    return table\n\ndef question(table):\n#     question_link = lambda y:y[\"href\"]\n#     question_text = lambda y:y.text\n    question_list = table.find_all(\"a\")\n    \n    ques_text = [question_list[i].text for i in range(4,len(question_list),2)]\n    ques_link = [question_list[i][\"href\"] for i in range(4,len(question_list),2)]\n    \n    return ques_text, ques_link\n\ndef views(table):\n    views = table.find_all(\"li\")\n    view_list = [views[i].text.split()[0] for i in range(1,len(views),2)]\n    \n    return view_list\n    \ndef scrape_list(page_number):\n    time.sleep(1)\n    soup = page_soup(page_number)\n    table = table_soup(soup)\n    ques_text,ques_link= question(table)\n    view = views(table)\n    return ques_text,ques_link,view\n\n\n    \n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Question Link, text and views Extraction\ndf = {\"question\":[],\"links\":[],\"views\":[]}\n\nfor page_number in range(231):\n    ques_text,ques_link,view = scrape_list(page_number)\n    \n    for i in range(len(ques_text)):\n        df[\"question\"].append(ques_text[i])\n        df[\"links\"].append(ques_link[i])\n        df[\"views\"].append(view[i])\n    \n        \ndf = pd.DataFrame.from_dict(df)\ndf\n    ","metadata":{"scrolled":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv(\"scrape.csv\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def answer_soup(url):\n    url = url\n    #Making a GET requests to fetch the raw HTML content\n    # headers = {\n    #     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n    # }\n    # headers={\n    #   'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n    #   'Accept-Language':'zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7',\n    #   'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'\n    # }\n    \n    try :\n        r = requests.get(url)\n        c = r.content\n        #Parsing the HTML content\n        soup = BeautifulSoup(c,\"html.parser\")\n        return soup\n    except:\n        print(\"|||||||||||||||||||Captcha Error|||||||||||||||||||||\")\n        pass\n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(\"scrape.csv\",index_col =0)\ndf.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def answer_soup(url,ques_number):\n    url = url\n    #Making a GET requests to fetch the raw HTML content\n    # headers = {\n    #     'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.12; rv:55.0) Gecko/20100101 Firefox/55.0',\n    # }\n    # headers={\n    #   'Accept':'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',\n    #   'Accept-Language':'zh-CN,zh;q=0.9,en;q=0.8,ja;q=0.7',\n    #   'User-Agent':'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36'\n    # }\n    r = requests.get(url)\n    c = r.content\n    #Parsing the HTML content\n    soup = BeautifulSoup(c,\"html.parser\")\n    \n    while soup.find(\"title\").text =='Verify, Please!':\n        alert = \"!\"*20,\"ques_number \",str(ques_number),\"\\t Captcha Error\",\"!\"*20\n        alert = colored(alert,\"red\",attrs=[\"reverse\",\"blink\"])\n        print(alert)\n        Audio(sound_file, autoplay=True)\n        time.sleep(30)\n        #Again Request That Page\n        r = requests.get(url)\n        c = r.content\n        #Parsing the HTML content\n        soup = BeautifulSoup(c,\"html.parser\")\n    \n    text = \"Question Number\\t\"+str(ques_number) +\"...\"\n    \n    text_color = colored(text, 'green', attrs=['reverse', 'blink'])\n    print(text_color)\n    \n    \n    return soup\n\ndef ques_answer(url,ques_number):\n    soup = answer_soup(url,ques_number)\n    soup = soup.find(\"article\")\n    div = soup.find_all(\"div\")\n    answer = soup.find(\"div\",{\"itemprop\":\"suggestedAnswer acceptedAnswer\"})\n    answer = answer.find(\"div\",{\"itemprop\":\"text\"})\n    ans = answer.text.strip()\n    \n    return ans\n    \n    ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_list = []\nfor i in range(4607):\n    ques_number = i\n    url = df[\"links\"].iloc[i]\n    answer_list.append(ques_answer(url,ques_number))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer.text.strip()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"answer_df = pd.DataFrame(answer_list)\nanswer_df.to_csv(\"answer_list\")\nanswer_list","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"answer_list\"] = x4\ndf.info()","metadata":{},"execution_count":null,"outputs":[]}]}